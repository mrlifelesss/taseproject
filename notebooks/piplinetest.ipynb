{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a91a25fe-29ed-47fd-9988-284f4d434afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chardet\n",
    "import math\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "\n",
    "# --- Step 1: Load any file smartly ---\n",
    "def smart_load_text(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        raw_data = f.read()\n",
    "    result = chardet.detect(raw_data)\n",
    "    encoding = result['encoding'] or \"utf-8\"\n",
    "    text = raw_data.decode(encoding, errors=\"replace\")\n",
    "    return text\n",
    "\n",
    "# --- Step 2: Extract text from HTML ---\n",
    "def extract_text_from_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    for unwanted in soup([\"script\", \"style\", \"head\", \"footer\", \"nav\"]):\n",
    "        unwanted.decompose()\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "    lines = [line.strip() for line in text.splitlines()]\n",
    "    return \"\\n\".join(line for line in lines if line)\n",
    "\n",
    "# --- Step 3: Clean Hebrew text ---\n",
    "import re\n",
    " \n",
    "def clean_hebrew_text(text):\n",
    "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"(?<=[^\\.\\!\\?:])\\n(?=[^\\n\\Wא-תa-zA-Z])\", \" \", text)\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n",
    "    text = re.sub(r\"(\\S)[ ]{3,}(\\S)\", r\"\\1 | \\2\", text)\n",
    "    text = re.sub(r\"^[•·●▪️✓✔▶►❖-]+[ \\t]+\", \"- \", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"[^\\x00-\\x7F\\u0590-\\u05FF\\d\\.\\,\\-\\:\\;\\|\\!\\?\\(\\)\\\"\\'\\n ]\", \" \", text)\n",
    "    text = re.sub(r\"[ \\t]+\\n\", \"\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "# --- Step 4: Count tokens ---\n",
    "def count_tokens(text, tokenizer):\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "# --- Step 5: Split text into chunks ---\n",
    "def split_into_chunks(text, tokenizer, max_tokens=7500):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_tokens):\n",
    "        chunk = tokens[i:i+max_tokens]\n",
    "        chunk_text = tokenizer.decode(chunk)\n",
    "        chunks.append(chunk_text)\n",
    "    return chunks\n",
    "\n",
    "# --- Step 6: Summarize one chunk ---\n",
    "def summarize_one_chunk(text, template, model, tokenizer, max_new_tokens=800, temperature=0.3):\n",
    "    prompt = f\"\"\"\n",
    "הטקסט הבא הוא הדוח שצריך לסכם:\n",
    "\n",
    "{text}\n",
    "\n",
    "אנא סכם את הדוח לפי התבנית הבאה:\n",
    "\n",
    "{template}\n",
    "\"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids\n",
    "    input_ids = input_ids.to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    torch.cuda.empty_cache()  # Free memory immediately after generation\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# --- Step 7: Master pipeline ---\n",
    "def summarize_file(file_path, template_path, model, tokenizer):\n",
    "    # Step A: Load file\n",
    "    raw_text = smart_load_text(file_path)\n",
    "\n",
    "    # Step B: If HTML, extract text\n",
    "    if file_path.endswith((\".html\", \".htm\")):\n",
    "        raw_text = extract_text_from_html(raw_text)\n",
    "\n",
    "    # Step C: Clean text\n",
    "    clean_text = clean_hebrew_text(raw_text)\n",
    "\n",
    "    # Step D: Load template\n",
    "    with open(template_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        template = f.read()\n",
    "\n",
    "    # Step E: Check if need to split\n",
    "    total_tokens = count_tokens(clean_text, tokenizer)\n",
    "\n",
    "    if total_tokens <= 8000:\n",
    "        chunks = [clean_text]\n",
    "    else:\n",
    "        chunks = split_into_chunks(clean_text, tokenizer)\n",
    "\n",
    "    # Step F: Summarize each chunk\n",
    "    summaries = []\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        print(f\"Summarizing chunk {idx+1}/{len(chunks)}...\")\n",
    "        summary = summarize_one_chunk(chunk, template, model, tokenizer)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    # Step G: Combine summaries\n",
    "    final_summary = \"\\n\\n\".join(summaries)\n",
    "    \n",
    "    return final_summary\n",
    "    \n",
    "def remove_after_signature(text, signature_marker=\"פרטי החותמים המורשים לחתום בשם התאגיד:\"):\n",
    "    \"\"\"\n",
    "    Removes everything in the text after the first occurrence of the signature marker.\n",
    "    If the marker is not found, returns the text unchanged.\n",
    "    \"\"\"\n",
    "    idx = text.find(signature_marker)\n",
    "    if idx != -1:\n",
    "        return text[:idx].strip()  # Keep only text before marker\n",
    "    else:\n",
    "        return text  # No marker found, return full text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04b393c2-5265-4a25-b9ce-ad0bd4680504",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Loading an AWQ quantized model requires auto-awq library (`pip install autoawq`)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Load tokenizer (normal)\u001b[39;00m\n\u001b[32m      7\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_path)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m.to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel loaded on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel.device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# 2. Call the pipeline\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mG:\\tase_project\\env\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mG:\\tase_project\\env\\Lib\\site-packages\\transformers\\modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mG:\\tase_project\\env\\Lib\\site-packages\\transformers\\modeling_utils.py:4228\u001b[39m, in \u001b[36mfrom_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4225\u001b[39m     error_msgs = _load_state_dict_into_model(model_to_load, state_dict, start_prefix)\n\u001b[32m   4226\u001b[39m     offload_index = None\n\u001b[32m   4227\u001b[39m else:\n\u001b[32m-> \u001b[39m\u001b[32m4228\u001b[39m     # Sharded checkpoint or whole but low_cpu_mem_usage==True\n\u001b[32m   4229\u001b[39m \n\u001b[32m   4230\u001b[39m     # This should always be a list but, just to be sure.\n\u001b[32m   4231\u001b[39m     if not isinstance(resolved_archive_file, list):\n\u001b[32m   4232\u001b[39m         resolved_archive_file = [resolved_archive_file]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mG:\\tase_project\\env\\Lib\\site-packages\\transformers\\quantizers\\quantizer_awq.py:50\u001b[39m, in \u001b[36mvalidate_environment\u001b[39m\u001b[34m(self, device_map, **kwargs)\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m: Loading an AWQ quantized model requires auto-awq library (`pip install autoawq`)"
     ]
    }
   ],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"G:/tase_project/models/dictalm2.0-AWQ\"\n",
    "\n",
    "# Load tokenizer (normal)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(\"cuda\")\n",
    "print(f\"Model loaded on {model.device}\")\n",
    "\n",
    "# 2. Call the pipeline\n",
    "final_summary = summarize_file(\n",
    "    file_path=\"../inputs/1660447.htm\",\n",
    "    template_path=\"שינוי החזקות בע נ.משרה.txt\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "# 3. Save or print\n",
    "with open(\"final_summary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(final_summary)\n",
    "\n",
    "print(\"✅ Summarization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7798d6eb-20cc-40e5-a59b-b5d635c1971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "testpath = \"../inputs/1660447.htm\"\n",
    "# After loading and extracting HTML\n",
    "html_content = smart_load_text(testpath)\n",
    "raw_html_text = extract_text_from_html(html_content)\n",
    "#clean_text = clean_hebrew_text(raw_html_text)\n",
    "\n",
    "# Remove unnecessary ending\n",
    "clean_text = remove_after_signature(raw_html_text)\n",
    "\n",
    "# Now clean_text is ready for token counting / summarizing\n",
    "\n",
    "# Save texty to a .txt file with UTF-8 encoding\n",
    "output_path = \"../outputs/1660447.txt\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9621804a-d24c-42f5-baad-c23a0155ca6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Loading an AWQ quantized model requires auto-awq library (`pip install autoawq`)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_path)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Important if model is quantized\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Tell it to load model.safetensors\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m.to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Model loaded on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel.device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mG:\\tase_project\\env\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mG:\\tase_project\\env\\Lib\\site-packages\\transformers\\modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mG:\\tase_project\\env\\Lib\\site-packages\\transformers\\modeling_utils.py:4228\u001b[39m, in \u001b[36mfrom_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4225\u001b[39m     error_msgs = _load_state_dict_into_model(model_to_load, state_dict, start_prefix)\n\u001b[32m   4226\u001b[39m     offload_index = None\n\u001b[32m   4227\u001b[39m else:\n\u001b[32m-> \u001b[39m\u001b[32m4228\u001b[39m     # Sharded checkpoint or whole but low_cpu_mem_usage==True\n\u001b[32m   4229\u001b[39m \n\u001b[32m   4230\u001b[39m     # This should always be a list but, just to be sure.\n\u001b[32m   4231\u001b[39m     if not isinstance(resolved_archive_file, list):\n\u001b[32m   4232\u001b[39m         resolved_archive_file = [resolved_archive_file]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mG:\\tase_project\\env\\Lib\\site-packages\\transformers\\quantizers\\quantizer_awq.py:50\u001b[39m, in \u001b[36mvalidate_environment\u001b[39m\u001b[34m(self, device_map, **kwargs)\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m: Loading an AWQ quantized model requires auto-awq library (`pip install autoawq`)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = \"G:/tase_project/models/dictalm2.0-AWQ\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=\"auto\",  # Important if model is quantized\n",
    "    use_safetensors=True  # Tell it to load model.safetensors\n",
    ").to(\"cuda\")\n",
    "\n",
    "print(f\"✅ Model loaded on {model.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cd90b7-4390-4a92-86ed-a211ca970e18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
