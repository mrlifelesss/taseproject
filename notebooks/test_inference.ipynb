{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0771f91-ed78-4720-9715-2e4c489ebcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_path = \"G:/tase_project/models/dictalm2.0\"\n",
    "\n",
    "# Load from local path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "device = \"cuda\" \n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True).to(device)\n",
    "print(f\"Model loaded on {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a23554c-59c2-4bc4-bdc8-45befda90346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=200, temperature=0.9, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generate text from DictaLM2.0 based on a given prompt.\n",
    "    \"\"\"\n",
    "    # Add \":\" and newline to hint the model to continue\n",
    "    prepared_prompt = prompt.strip() + \":\\n\"\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prepared_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p\n",
    "        )\n",
    "\n",
    "    # Decode the output\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627bfaf4-93a8-4126-8636-f77b13f5964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e7dc689-8e41-4d49-8dac-90d7c0e38b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def smart_split_text_by_tokens(text, tokenizer, max_tokens=300):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        current_chunk.append(token)\n",
    "        if len(current_chunk) >= max_tokens:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = []\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    decoded_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n",
    "    return decoded_chunks\n",
    "\n",
    "def summarize_pdf_to_text(pdf_path, model = model, tokenizer = tokenizer, device = device, chunk_tokens=300, summary_tokens=200):\n",
    "    # 1. Extract text from the PDF\n",
    "    full_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # 2. Split text into smaller chunks\n",
    "    chunks = smart_split_text_by_tokens(full_text, tokenizer, max_tokens=chunk_tokens)\n",
    "    \n",
    "    # 3. Summarize each chunk\n",
    "    summaries = []\n",
    "    for chunk_text in chunks:\n",
    "        prompt = f\"הנה קטע טקסט. סכם אותו בקצרה בעברית:\\n\\n{chunk_text}\\n\\nסיכום:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=summary_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "\n",
    "        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        summaries.append(summary)\n",
    "    \n",
    "    # 4. Combine all summaries into one text\n",
    "    full_summary = \"\\n\\n\".join(summaries)\n",
    "    \n",
    "    return full_summary\n",
    "\n",
    "def summarize_text_to_text(full_text, model, tokenizer, device, chunk_tokens=300, summary_tokens=200):\n",
    "    \n",
    "    # 2. Split text into smaller chunks\n",
    "    chunks = smart_split_text_by_tokens(full_text, tokenizer, max_tokens=chunk_tokens)\n",
    "    \n",
    "    # 3. Summarize each chunk with a progress bar\n",
    "    summaries = []\n",
    "    \n",
    "    for chunk_text in tqdm(chunks, desc=\"Summarizing chunks\", unit=\"chunk\"):\n",
    "        prompt = f\"הנה קטע טקסט. סכם אותו בקצרה בעברית:\\n\\n{chunk_text}\\n\\nסיכום:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=summary_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "\n",
    "        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        summaries.append(summary)\n",
    "    \n",
    "    # 4. Combine all summaries into one text\n",
    "    full_summary = \"\\n\\n\".join(summaries)\n",
    "    \n",
    "    return full_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65960e8f-1cea-4c27-8dce-73274da8bc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume model, tokenizer, device already loaded\n",
    "pdf_path = r\"../inputs/1660447.htm\"\n",
    "\n",
    "summary = summarize_pdf_to_text(\n",
    "    pdf_path,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    chunk_tokens=300,\n",
    "    summary_tokens=200\n",
    ")\n",
    "\n",
    "# Print the summary\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bff276-5c28-4ee8-88b6-9acc4a3e1947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_extracted_text(text):\n",
    "    # 1. Remove multiple spaces and tabs\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "\n",
    "    # 2. Remove multiple newlines (more than 2) ➔ collapse to just one\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n",
    "\n",
    "    # 3. Remove newlines inside paragraphs\n",
    "    # (Keep newlines only after sentence ends . ! ?)\n",
    "    text = re.sub(r\"(?<![\\.\\!\\?])\\n(?!\\n)\", \" \", text)\n",
    "\n",
    "    # 4. Remove weird characters (optional: you can expand this as needed)\n",
    "    text = re.sub(r\"[^\\x00-\\x7F\\u0590-\\u05FF]+\", \" \", text)  # Keep ASCII + Hebrew\n",
    "\n",
    "    # 5. Strip leading/trailing spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def clean_hebrew_pdf_text(text):\n",
    "    # 1. Normalize line endings\n",
    "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "\n",
    "    # 2. Remove multiple tabs/spaces\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "\n",
    "    # 3. Merge broken lines inside paragraphs:\n",
    "    # If a line ends without . ? ! or : and the next line starts with a lowercase or Hebrew letter — merge them\n",
    "    text = re.sub(r\"(?<=[^\\.\\!\\?:])\\n(?=[^\\n\\Wא-תa-zA-Z])\", \" \", text)\n",
    "\n",
    "    # 4. Collapse multiple newlines into a single one (for clean paragraph breaks)\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n",
    "\n",
    "    # 5. Handle tables:\n",
    "    # Sometimes tables split rows with too many spaces or newlines inside a row — try to fix\n",
    "    text = re.sub(r\"(\\S)[ ]{3,}(\\S)\", r\"\\1 | \\2\", text)  # Assume 3+ spaces = table column gap\n",
    "    text = re.sub(r\"\\n(?=\\S+ [|] \\S+)\", \" \", text)  # Merge table lines if they continue\n",
    "\n",
    "    # 6. Normalize bullets:\n",
    "    # Replace various Hebrew/English bullets with a standard dash\n",
    "    text = re.sub(r\"^[•·●▪️✓✔▶►❖-]+[ \\t]+\", \"- \", text, flags=re.MULTILINE)\n",
    "\n",
    "    # 7. Remove strange Unicode artifacts (keep Hebrew, English, numbers, and basic symbols)\n",
    "    text = re.sub(r\"[^\\x00-\\x7F\\u0590-\\u05FF\\d\\.\\,\\-\\:\\;\\|\\!\\?\\(\\)\\\"\\'\\n ]\", \" \", text)\n",
    "\n",
    "    # 8. Strip trailing spaces at end of lines\n",
    "    text = re.sub(r\"[ \\t]+\\n\", \"\\n\", text)\n",
    "\n",
    "    # 9. Final trim\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "683f8d94-4cb7-43ab-ba91-b9bae343b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import chardet\n",
    "\n",
    "def smart_load_html(file_path):\n",
    "    # Step 1: Read the raw bytes first\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        raw_data = f.read()\n",
    "\n",
    "    # Step 2: Detect encoding\n",
    "    result = chardet.detect(raw_data)\n",
    "    detected_encoding = result['encoding']\n",
    "\n",
    "    # Step 3: Use detected encoding to decode properly\n",
    "    try:\n",
    "        text = raw_data.decode(detected_encoding)\n",
    "    except UnicodeDecodeError:\n",
    "        # fallback: force decode as windows-1255 if detection failed\n",
    "        text = raw_data.decode(\"windows-1255\", errors=\"replace\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_text_from_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    # Remove unwanted elements (optional: scripts, styles)\n",
    "    for unwanted in soup([\"script\", \"style\", \"head\", \"footer\", \"nav\"]):\n",
    "        unwanted.decompose()\n",
    "\n",
    "    # Get raw text\n",
    "    text = soup.get_text(separator=\"\\n\")  # Insert \\n where tags like <p>, <div> end\n",
    "\n",
    "    # Remove excessive newlines\n",
    "    lines = [line.strip() for line in text.splitlines()]\n",
    "    text = \"\\n\".join(line for line in lines if line)\n",
    "\n",
    "    return text\n",
    "import re\n",
    "\n",
    "def clean_hebrew_html_text(text):\n",
    "    # 1. Normalize line endings\n",
    "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "\n",
    "    # 2. Remove multiple spaces and tabs\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "\n",
    "    # 3. Merge broken lines inside paragraphs\n",
    "    text = re.sub(r\"(?<=[^\\.\\!\\?:])\\n(?=[^\\n\\Wא-תa-zA-Z])\", \" \", text)\n",
    "\n",
    "    # 4. Collapse multiple newlines into one\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n",
    "\n",
    "    # 5. Handle table columns: 3+ spaces mean new table column\n",
    "    text = re.sub(r\"(\\S)[ ]{3,}(\\S)\", r\"\\1 | \\2\", text)\n",
    "\n",
    "    # 6. Normalize bullets\n",
    "    text = re.sub(r\"^[•·●▪️✓✔▶►❖-]+[ \\t]+\", \"- \", text, flags=re.MULTILINE)\n",
    "\n",
    "    # 7. Remove strange Unicode artifacts (keep Hebrew, English, digits, symbols)\n",
    "    text = re.sub(r\"[^\\x00-\\x7F\\u0590-\\u05FF\\d\\.\\,\\-\\:\\;\\|\\!\\?\\(\\)\\\"\\'\\n ]\", \" \", text)\n",
    "\n",
    "    # 8. Strip trailing spaces\n",
    "    text = re.sub(r\"[ \\t]+\\n\", \"\\n\", text)\n",
    "\n",
    "    # 9. Final trim\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57761471-9808-42ca-8ed0-d3b8dd5e2b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HTML\n",
    "html_path = r\"../inputs/1660447.htm\"\n",
    "\n",
    "html_content = smart_load_html(html_path)\n",
    "\n",
    "# Step 1: Extract text\n",
    "raw_text = extract_text_from_html(html_content)\n",
    "\n",
    "# Step 2: Clean text\n",
    "clean_text = clean_hebrew_html_text(raw_text)\n",
    "\n",
    "# Now you can save or send it to the model\n",
    "summary = summarize_text_to_text(\n",
    "    clean_text,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    chunk_tokens=300,\n",
    "    summary_tokens=200\n",
    ")\n",
    "\n",
    "# Print the summary\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8105aa5c-16cf-443d-a13b-acaa7b255ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def summarize_with_template(\n",
    "    input_text_or_path,\n",
    "    template_path,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=800,\n",
    "    temperature=0.3\n",
    "):\n",
    "    \"\"\"\n",
    "    Summarizes input text according to a template.\n",
    "\n",
    "    Parameters:\n",
    "    - input_text_or_path: str or Path (either raw text or a file path)\n",
    "    - template_path: str or Path to template.txt\n",
    "    - model: Hugging Face model object\n",
    "    - tokenizer: Hugging Face tokenizer object\n",
    "    - max_new_tokens: number of tokens to generate\n",
    "    - temperature: creativity control (lower = more strict)\n",
    "    \n",
    "    Returns:\n",
    "    - The generated summary (str)\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Load input text if given a path\n",
    "    if isinstance(input_text_or_path, str) and input_text_or_path.endswith((\".txt\", \".htm\", \".html\")):\n",
    "        # Assume it's a file path, read content\n",
    "        with open(input_text_or_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            input_text = f.read()\n",
    "    else:\n",
    "        # Otherwise assume it's already clean text (str)\n",
    "        input_text = input_text_or_path\n",
    "\n",
    "    # Step 2: Load template\n",
    "    with open(template_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        template = f.read()\n",
    "\n",
    "    # Step 3: Build prompt\n",
    "    prompt = f\"\"\"\n",
    "הטקסט הבא הוא הדוח שצריך לסכם בעברית:\n",
    "\n",
    "{input_text}\n",
    "\n",
    "אנא סכם את הדוח לפי התבנית הבאה:\n",
    "\n",
    "{template}\n",
    "\"\"\"\n",
    "\n",
    "    # Step 4: Tokenize\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids\n",
    "\n",
    "    # Step 5: Generate\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    # Step 6: Decode\n",
    "    summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09975ab-ac18-4fa1-8bf9-0a6c9ca228d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = r\"../inputs/אבגד מקוצר מאוד.pdf\"\n",
    "template_path = r\"../inputs/שינוי החזקות בע נ.משרהץ.txt\"\n",
    "# After extracting from PDF\n",
    "raw_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Clean\n",
    "clean_text = clean_hebrew_pdf_text(raw_text)\n",
    "summary = summarize_with_template(clean_text,template_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
