{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb190ae-c06e-4da0-9575-5a6d530b3aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers accelerate torch safetensors boto3 chardet beautifulsoup4 pdfplumber pytesseract  google.cloud.aiplatform httpx PyPDF2 google.genai google\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d4f05d0-2ff1-43a1-92ae-a84c6a203dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.conda/envs/gemini-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import re\n",
    "import chardet\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import botocore.exceptions\n",
    "import io\n",
    "from PyPDF2 import PdfReader\n",
    "from botocore.exceptions import ClientError\n",
    "import pdfplumber\n",
    "import sys\n",
    "import google.generativeai as genai\n",
    "import google.generativeai.types as types\n",
    "import httpx # Keep if used elsewhere\n",
    "import os\n",
    "import tempfile # Import the tempfile module\n",
    "import mimetypes # Import mimetypes for fallback      \n",
    "import urllib.parse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "249ef741-add9-40a4-b2a3-b75ed5d01601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chardet in /opt/conda/lib/python3.12/site-packages (5.2.0)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc664b9-9cc4-4adb-95a1-92f507cb8e3c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- S3 Settings ---\n",
    "s3 = boto3.client(\"s3\")\n",
    "bucket = \"summery-prompts-data\"\n",
    "\n",
    "# --- Bedrock Jamba v1.5 Mini Config ---\n",
    "bedrock = boto3.client(\"bedrock-runtime\")\n",
    "MODEL_ID = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "CONTENT_TYPE = \"application/json\"\n",
    "ACCEPT = \"application/json\"\n",
    "\n",
    "# --- Bedrock Invocation ---\n",
    "def invoke_model_bedrock(prompt: str,\n",
    "                         max_tokens: int = 1024,\n",
    "                         temperature: float = 0.1,\n",
    "                         top_p: float = 0.95,\n",
    "                         anthropic_version: str = \"bedrock-2023-05-31\",\n",
    "                         max_retries: int = 6,\n",
    "                         base_backoff: float = 1.0) -> str:\n",
    "    \"\"\"\n",
    "    Invoke Bedrock with retry/backoff on ThrottlingException.\n",
    "    \"\"\"\n",
    "    is_anthropic = \"anthropic\" in MODEL_ID.lower()\n",
    "\n",
    "    # Build the request body based on model type\n",
    "    if is_anthropic:\n",
    "        body = {\n",
    "            \"anthropic_version\": anthropic_version,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "        }\n",
    "    else:\n",
    "        body = {\n",
    "            \"input\": prompt,\n",
    "            \"maxTokens\": max_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"topP\": top_p\n",
    "        }\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            resp = bedrock.invoke_model(\n",
    "                body=json.dumps(body),\n",
    "                modelId=MODEL_ID,\n",
    "                contentType=\"application/json\",\n",
    "                accept=\"application/json\"\n",
    "            )\n",
    "            data = json.loads(resp[\"body\"].read())\n",
    "            # parse response\n",
    "            if is_anthropic:\n",
    "                # handle both choices and single‐message formats\n",
    "                if \"choices\" in data:\n",
    "                    return data[\"choices\"][0][\"message\"][\"content\"]\n",
    "                else:\n",
    "                    blocks = data.get(\"content\", [])\n",
    "                    return \"\".join(b.get(\"text\", \"\") for b in blocks if b.get(\"type\") == \"text\")\n",
    "            else:\n",
    "                return data.get(\"generatedText\") or data.get(\"outputs\", [None])[0]\n",
    "\n",
    "        except ClientError as e:\n",
    "            code = e.response.get(\"Error\", {}).get(\"Code\", \"\")\n",
    "            # specifically catch throttling\n",
    "            if code == \"ThrottlingException\" and attempt < max_retries:\n",
    "                # Exponential backoff with jitter\n",
    "                backoff = base_backoff * (2 ** (attempt - 1))\n",
    "                sleep_time = backoff * random.uniform(0.5, 1.5)\n",
    "                print(f\"Throttled (attempt {attempt}/{max_retries}). Sleeping {sleep_time:.1f}s...\")\n",
    "                time.sleep(sleep_time)\n",
    "                continue\n",
    "            # re-raise all other errors or if we're out of retries\n",
    "            raise\n",
    "\n",
    "    # If we somehow exit loop without returning or raising, fail explicitly\n",
    "    raise RuntimeError(\"invoke_model_bedrock: exceeded max retries without success\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "844361d6-074e-4320-a67e-9b14fc44d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform import schema\n",
    "from google.cloud.aiplatform.gapic import PredictionServiceClient\n",
    "\n",
    "import httpx\n",
    "# --- S3 Settings ---\n",
    "s3 = boto3.client(\"s3\")\n",
    "bucket = \"summery-prompts-data\"\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyA1NptQkNBxjgFG9Lop_NrfjVKq7-qg5HU\")\n",
    "def generate_with_gemini(\n",
    "    prompt: str,\n",
    "    key: str, # S3 key\n",
    "    model_name: str = \"gemini-1.5-flash-latest\",\n",
    "    s3_bucket: str = \"summery-prompts-data\" # Make bucket configurable\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generates content using Gemini with File API, counting tokens first.\n",
    "    Handles S3 file reading, File API upload/processing/cleanup via temp file.\n",
    "\n",
    "    Args:\n",
    "        prompt: The text prompt.\n",
    "        key: The S3 key for the input document.\n",
    "        model_name: The name of the Gemini model to use.\n",
    "        s3_bucket: The S3 bucket name.\n",
    "\n",
    "    Returns:\n",
    "        The generated summary text or an error message string.\n",
    "    \"\"\"\n",
    "    uploaded_file = None\n",
    "    temp_file_path = None  # Keep track of temp file path for logging\n",
    "    try:\n",
    "        print(f\"--- Starting generation for S3 key: {key} ---\")\n",
    "        print(f\"Using model: {model_name}\")\n",
    "        actual_model_instance = genai.GenerativeModel(model_name)\n",
    "\n",
    "        # 1. Read data from S3\n",
    "        print(f\"Reading from s3://{s3_bucket}/{key}\")\n",
    "        resp = s3.get_object(Bucket=s3_bucket, Key=key)\n",
    "        doc_data = resp[\"Body\"].read()\n",
    "        print(f\"Read {len(doc_data)} bytes from S3.\")\n",
    "\n",
    "        # 2. Detect MIME type\n",
    "        file_ending = detect_file_type(key)\n",
    "        mime_type = {\n",
    "            \"pdf\": \"application/pdf\",\n",
    "            \"txt\": \"text/plain\",\n",
    "            \"html\": \"text/html\"\n",
    "        }.get(file_ending)\n",
    "\n",
    "        if not mime_type:\n",
    "            mime_type, _ = mimetypes.guess_type(key)\n",
    "            if not mime_type:\n",
    "                error_msg = f\"Error: Could not determine MIME type for key: {key}. Unsupported file type: {file_ending!r}\"\n",
    "                print(error_msg)\n",
    "                return error_msg\n",
    "            else:\n",
    "                print(f\"Used fallback mimetypes guess: {mime_type}\")\n",
    "\n",
    "        print(f\"Detected MIME type: {mime_type}\")\n",
    "\n",
    "        # --- Use File API via Temporary File ---\n",
    "        # 3. Create a temporary file and write bytes to it\n",
    "        #    Using 'with' ensures the file is deleted automatically afterward\n",
    "        #    'delete=False' needed on Windows if file needs to be opened again by name\n",
    "        #    Suffix helps `upload_file` potentially guess mime type if needed\n",
    "        file_suffix = f\".{file_ending}\" if file_ending else None\n",
    "        with tempfile.NamedTemporaryFile(suffix=file_suffix, delete=False) as temp_file:\n",
    "            temp_file.write(doc_data)\n",
    "            temp_file_path = temp_file.name # Get the path to the temp file\n",
    "            print(f\"Data written to temporary file: {temp_file_path}\")\n",
    "\n",
    "        # 4. Upload the temporary file using its PATH\n",
    "        print(f\"Uploading temporary file: {temp_file_path}...\")\n",
    "        display_name = key.split('/')[-1] if '/' in key else key\n",
    "        uploaded_file = genai.upload_file(\n",
    "            path=temp_file_path,        # <--- PASS THE FILE PATH (STR)\n",
    "            display_name=display_name,\n",
    "            mime_type=mime_type         # Provide mime_type to be certain\n",
    "        )\n",
    "        print(f\"Uploaded file: {uploaded_file.name} ({uploaded_file.display_name})\")\n",
    "        print(f\"Initial state: {uploaded_file.state.name}\")\n",
    "        # --- End of Temp File Usage Section ---\n",
    "\n",
    "        # 5. Wait for the file to be processed (same as before)\n",
    "        while uploaded_file.state.name == \"PROCESSING\":\n",
    "            print(\"Waiting for file processing (state: PROCESSING)...\")\n",
    "            time.sleep(5)\n",
    "            uploaded_file = genai.get_file(uploaded_file.name)\n",
    "\n",
    "        if uploaded_file.state.name != \"ACTIVE\":\n",
    "            error_msg = f\"Error: File processing failed for {uploaded_file.name}. Final state: {uploaded_file.state.name}\"\n",
    "            print(error_msg)\n",
    "            return error_msg\n",
    "\n",
    "        print(f\"File is ACTIVE: {uploaded_file.name}\")\n",
    "\n",
    "        # 6. Prepare contents using the file URI (same as before)\n",
    "        file_part = genai.Part.from_uri(  # <--- CORRECTED: Use genai.Part\n",
    "            uri=uploaded_file.uri,\n",
    "            mime_type=uploaded_file.mime_type\n",
    "        )\n",
    "        contents_to_send = [file_part, prompt]\n",
    "\n",
    "        # 7. Count tokens (same as before)\n",
    "        print(\"Counting tokens...\")\n",
    "        token_response = actual_model_instance.count_tokens(contents=contents_to_send)\n",
    "        total_tokens = token_response.total_tokens\n",
    "        print(f\"Total tokens for the input: {total_tokens}\")\n",
    "\n",
    "        TOKEN_LIMIT = 1_000_000\n",
    "        if total_tokens > TOKEN_LIMIT:\n",
    "            error_msg = f\"Error: Input ({total_tokens} tokens) exceeds model limit ({TOKEN_LIMIT} tokens) for {uploaded_file.name}.\"\n",
    "            print(error_msg)\n",
    "            return error_msg\n",
    "\n",
    "        # 8. Call Gemini for generation (same as before)\n",
    "        print(\"Token count within limits. Proceeding with generation...\")\n",
    "        response = actual_model_instance.generate_content(\n",
    "            contents=contents_to_send,\n",
    "        )\n",
    "        print(\"Generation complete.\")\n",
    "\n",
    "        # 9. Extract text (same as before)\n",
    "        print(\"Extracting text from response...\")\n",
    "        if not response.candidates:\n",
    "             print(\"Warning: No candidates found in the response.\")\n",
    "             if hasattr(response, 'prompt_feedback'): print(f\"Prompt Feedback: {response.prompt_feedback}\")\n",
    "             return \"Error: No response generated (no candidates).\"\n",
    "\n",
    "        candidate = response.candidates[0]\n",
    "        if not hasattr(candidate, 'content') or not hasattr(candidate.content, 'parts'):\n",
    "             print(\"Warning: Response candidate structure is missing content or parts.\")\n",
    "             print(f\"Candidate: {candidate}\")\n",
    "             return \"Error: Unexpected response structure.\"\n",
    "\n",
    "        result_text = \"\".join(part.text for part in candidate.content.parts if hasattr(part, \"text\"))\n",
    "        print(\"Text extracted successfully.\")\n",
    "        return result_text\n",
    "\n",
    "    except genai.types.BlockedPromptException as e:\n",
    "        error_msg = f\"Error: Generation failed because the prompt was blocked. {e}\"\n",
    "        print(error_msg)\n",
    "        return error_msg\n",
    "    except Exception as e:\n",
    "        error_msg = f\"An unexpected error occurred in generate_with_gemini for key {key}: {e}\"\n",
    "        print(error_msg)\n",
    "        # import traceback # Uncomment for detailed debugging if needed\n",
    "        # print(traceback.format_exc())\n",
    "        return error_msg\n",
    "\n",
    "    finally:\n",
    "        # --- Cleanup Steps ---\n",
    "        # Delete the temporary file from local disk\n",
    "        if temp_file_path and os.path.exists(temp_file_path):\n",
    "            try:\n",
    "                print(f\"Attempting to delete temporary file: {temp_file_path}\")\n",
    "                os.remove(temp_file_path)\n",
    "                print(f\"Successfully deleted temporary file: {temp_file_path}\")\n",
    "            except Exception as del_err:\n",
    "                print(f\"Warning: Failed to delete temporary file {temp_file_path}: {del_err}\")\n",
    "\n",
    "        # Delete the file from Gemini File API storage\n",
    "        if uploaded_file:\n",
    "            try:\n",
    "                print(f\"Attempting to delete uploaded file from Gemini: {uploaded_file.name}\")\n",
    "                genai.delete_file(uploaded_file.name)\n",
    "                print(f\"Successfully deleted file from Gemini: {uploaded_file.name}\")\n",
    "            except Exception as del_err:\n",
    "                print(f\"Warning: Failed to delete file {uploaded_file.name} from Gemini: {del_err}\")\n",
    "        print(f\"--- Finished processing S3 key: {key} ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37120669-7b57-4414-a373-5b28fdcba733",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "bucket = \"summery-prompts-data\"\n",
    "docs_prefix = \"שינוי אחזקות בעלי עניין/Docs/\"\n",
    "template_key = \"שינוי אחזקות בעלי עניין/תנועות בעלי עניין2.txt\"\n",
    "region = s3.meta.region_name\n",
    "# paginator in case you have many files\n",
    "paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "for page in paginator.paginate(Bucket=bucket, Prefix=docs_prefix):\n",
    "    for obj in page.get(\"Contents\", []):\n",
    "        key = obj[\"Key\"]\n",
    "        if not key.lower().endswith((\".pdf\")):\n",
    "            continue\n",
    "\n",
    "        # extract the numeric ID (basename without extension)\n",
    "        file_id = key.rsplit(\"/\", 1)[-1].rsplit(\".\", 1)[0]\n",
    "\n",
    "        input_key = key\n",
    "        output_key = f\"שינוי אחזקות בעלי עניין/Outputs/G{file_id}_Output.txt\"\n",
    "        # percent-encode the key (but leave \"/\" alone so we keep folders)\n",
    "        encoded_key = urllib.parse.quote(key, safe=\"/\")\n",
    "        url = f\"https://{bucket}.s3.{region}.amazonaws.com/{encoded_key}\"\n",
    "        print(f\"Processing {input_key} → {output_key}\")\n",
    "        summarize_s3_file_gem(input_key,template_key,output_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "224368b4-53d2-4814-a5b5-025924c82243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_s3_file_gem(key, template_key, output_key):\n",
    "    template = s3_read_text(template_key)\n",
    "    prompt = f\"\"\"\n",
    "אנא סכם את הדוח לפי התבנית הבאה:\n",
    "{template}\n",
    "אל תשתמש באנגלית בכלל\n",
    "\"\"\"\n",
    "    final_summary = generate_with_gemini(prompt, key)\n",
    "    s3_write_text(output_key, final_summary)\n",
    "    print(f\"✅ Summary written to s3://{bucket}/{output_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfba6f4-6864-4bf4-9151-af34a2b36c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import httpx\n",
    "client = genai.Client(api_key=\"AIzaSyA1NptQkNBxjgFG9Lop_NrfjVKq7-qg5HU\")\n",
    "\n",
    "def g0enerate_with_gemini(prompt: str, model: str = \"gemini-1.5-flash-8b\",url) -> str:\n",
    "    ending = detect_file_type(url)\n",
    "    doc_data = httpx.get(url).content\n",
    "    if( ending == \"pdf\"):\n",
    "        response = client.models.generate_content(\n",
    "            model=model,   contents=[\n",
    "                types.Part.from_bytes(\n",
    "                data=doc_data,\n",
    "                mime_type='application/pdf',\n",
    "            ),\n",
    "              prompt])\n",
    "        )\n",
    "        return response.predictions[0][\"content\"]\n",
    "    else if(ending == \"txt\"):\n",
    "                response = client.models.generate_content(\n",
    "            model=model,   contents=[\n",
    "                types.Part.from_bytes(\n",
    "                data=doc_data,\n",
    "                mime_type='text/plain',\n",
    "            ),\n",
    "              prompt])\n",
    "        )\n",
    "        return response.predictions[0][\"content\"]\n",
    "    else if(ending == html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04145f29-42db-477c-bbcb-22e67d5ad201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_with_textract(pdf_bytes: bytes) -> str:\n",
    "    textract = boto3.client('textract')\n",
    "    # For multi‐page PDFs you really should use StartDocumentTextDetection + GetDocumentTextDetection\n",
    "    # but for simplicity here we'll assume single‐page or small docs:\n",
    "    response = textract.detect_document_text(Document={'Bytes': pdf_bytes})\n",
    "    lines = [\n",
    "        block['DetectedText']\n",
    "        for block in response['Blocks']\n",
    "        if block['BlockType'] == 'LINE'\n",
    "    ]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def extract_text_from_pdf_bytes(pdf_bytes: bytes,\n",
    "                               ocr_lang: str = 'eng',\n",
    "                               use_textract: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    1) Extracts embedded text via pdfplumber  \n",
    "    2) Falls back to local Tesseract OCR  \n",
    "    3) If Tesseract isn’t installed or `use_textract=True`, calls Amazon Textract\n",
    "    \"\"\"\n",
    "    text_pages = []\n",
    "    # If you want *only* Textract, you can short‐circuit here:\n",
    "    if use_textract:\n",
    "        return ocr_with_textract(pdf_bytes)\n",
    "\n",
    "    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if (page_text and page_text.strip() )or sys.getsizeof(input_text) < 100:\n",
    "                text_pages.append(page_text)\n",
    "            else:\n",
    "                    return ocr_with_textract(pdf_bytes)\n",
    "\n",
    "    return \"\\n\\n\".join(text_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cee807d3-2375-40a0-8898-a302edf4a571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Text Utilities ---\n",
    "import chardet\n",
    "\n",
    "def extract_text_from_html(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "def extract_text_from_pdf_bytes(pdf_bytes: bytes) -> str:\n",
    "    text = []\n",
    "    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text.append(page.extract_text() or \"\")\n",
    "    return \"\\n\\n\".join(text)\n",
    "\n",
    "def s3_read_text(key: str) -> str:\n",
    "    \"\"\"\n",
    "    Downloads an S3 object and extracts text depending on file type:\n",
    "    - PDF: uses pdfplumber\n",
    "    - HTML: parses via BeautifulSoup\n",
    "    - Other: detects encoding and decodes as plain text\n",
    "    \"\"\"\n",
    "    raw_bytes = s3.get_object(Bucket=bucket, Key=key)[\"Body\"].read()\n",
    "\n",
    "    # Handle PDF\n",
    "    if key.lower().endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf_bytes(raw_bytes)\n",
    "\n",
    "    # Detect encoding\n",
    "    guess = chardet.detect(raw_bytes)\n",
    "    encoding = guess[\"encoding\"] or \"utf-8\"\n",
    "\n",
    "    text = raw_bytes.decode(encoding, errors=\"replace\")\n",
    "\n",
    "    # Handle HTML\n",
    "    if key.lower().endswith((\".htm\", \".html\")):\n",
    "        return extract_text_from_html(text)\n",
    "\n",
    "    # Plain text (txt, or fallback)\n",
    "    return text\n",
    "    \n",
    "def s3_write_text(key, content):\n",
    "    # Add BOM + RLE + PDF for full RTL enforcement\n",
    "    rtl_wrapped = \"\\u202B\" + content + \"\\u202C\"\n",
    "    bom_prefixed = \"\\ufeff\" + rtl_wrapped\n",
    "    s3.put_object(Bucket=bucket, Key=key, Body=bom_prefixed.encode(\"utf-8\"))\n",
    "\n",
    "def smart_load_text(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        raw_data = f.read()\n",
    "    encoding = chardet.detect(raw_data)['encoding'] or \"utf-8\"\n",
    "    return raw_data.decode(encoding, errors=\"replace\")\n",
    "\n",
    "def extract_text_from_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    for tag in soup([\"script\", \"style\", \"head\", \"footer\", \"nav\"]):\n",
    "        tag.decompose()\n",
    "    return \"\\n\".join([line.strip() for line in soup.get_text(\"\\n\").splitlines() if line.strip()])\n",
    "\n",
    "def clean_hebrew_text(text):\n",
    "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"(?<=[^\\.\\!\\?:])\\n(?=[^\\n\\Wא-תa-zA-Z])\", \" \", text)\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n",
    "    text = re.sub(r\"(\\S)[ ]{3,}(\\S)\", r\"\\1 | \\2\", text)\n",
    "    text = re.sub(r\"^[\\u2022\\u25CF\\u25AA\\u2713\\u2714\\u25B6\\u25BA\\u2756\\-]+[ \\t]+\", \"- \", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"[^\\x00-\\x7F\\u0590-\\u05FF\\d\\.\\,\\-\\:\\;\\|\\!\\?\\(\\)\\\"\\'\\n ]\", \" \", text)\n",
    "    text = re.sub(r\"[ \\t]+\\n\", \"\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def split_into_chunks(text, max_chars=4000):\n",
    "    paragraphs = text.split(\"\\n\")\n",
    "    chunks, chunk = [], \"\"\n",
    "    for para in paragraphs:\n",
    "        if len(chunk) + len(para) < max_chars:\n",
    "            chunk += para + \"\\n\"\n",
    "        else:\n",
    "            chunks.append(chunk.strip())\n",
    "            chunk = para + \"\\n\"\n",
    "    if chunk:\n",
    "        chunks.append(chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "def summarize_one_chunk(text, template):\n",
    "    prompt = f\"\"\"\n",
    "אנא סכם את הדוח לפי התבנית הבאה:\n",
    "{template}\n",
    "אל תשתמש באנגלית בכלל\n",
    "\"\"\"\n",
    "  #{template}\n",
    "    return generate_with_gemini(prompt) \n",
    "\n",
    "def summarize_s3_file(input_key, template_key, output_key):\n",
    "    input_text = s3_read_text(input_key)\n",
    "    template = s3_read_text(template_key)\n",
    "\n",
    "    if input_key.endswith((\".html\", \".htm\")):\n",
    "        input_text = extract_text_from_html(input_text)\n",
    "\n",
    "    clean_text = clean_hebrew_text(input_text)\n",
    "    chunks = split_into_chunks(clean_text)\n",
    "\n",
    "    summaries = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Summarizing chunk {i+1}/{len(chunks)}...\")\n",
    "        summary = summarize_one_chunk(chunk, template)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    \n",
    "    final_summary = \"‫\" + \"\\n\\n\".join(summaries) + \"‬\"  # Force RTL (embedding)  # Add RTL marker\n",
    "    s3_write_text(output_key, final_summary)\n",
    "    print(f\"✅ Summary written to s3://{bucket}/{output_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f839d3dc-9fbe-4983-84c1-bbb44f3169bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from pathlib import Path\n",
    "\n",
    "def detect_file_type(url: str, head_fallback: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Returns one of: \"pdf\", \"txt\", \"html\", or \"unknown\".\n",
    "    If head_fallback=True, does a HTTP HEAD to inspect Content-Type when extension is ambiguous.\n",
    "    \"\"\"\n",
    "    # 1) try extension from the path\n",
    "    path = urlparse(url).path         # e.g. \"/folder/file.pdf\"\n",
    "    ext  = Path(path).suffix.lower()  # e.g. \".pdf\"\n",
    "    if ext == \".pdf\":\n",
    "        return \"pdf\"\n",
    "    if ext == \".txt\":\n",
    "        return \"txt\"\n",
    "    if ext in (\".htm\", \".html\"):\n",
    "        return \"html\"\n",
    "\n",
    "    # 2) optional HEAD request fallback\n",
    "    if head_fallback:\n",
    "        try:\n",
    "            resp = requests.head(url, allow_redirects=True, timeout=5)\n",
    "            ctype = resp.headers.get(\"Content-Type\", \"\").lower()\n",
    "            if \"pdf\" in ctype:\n",
    "                return \"pdf\"\n",
    "            if \"html\" in ctype:\n",
    "                return \"html\"\n",
    "            if \"text\" in ctype:\n",
    "                return \"txt\"\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return \"unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5f0f46-0637-4a59-8d84-0ddfccdb1b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5368a5f-1b03-4bc9-ac08-6546ffec69cd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def estimate_token_count(text):\n",
    "    # 1 token ≈ 4 characters in English; Hebrew is usually more compressed\n",
    "    # For Hebrew, a safer estimate is ~1 token per 1.3 words\n",
    "    words = text.split()\n",
    "    return int(len(words) * 1.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a10541-e1ee-4452-a1e6-68e22ea578af",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_s3_file_gem(input_key,template_key,output_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5ac16a-af84-46d2-aa3d-7c388822e82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(f\"Using Python executable: {sys.executable}\")\n",
    "!{sys.executable} -m pip install --force-reinstall --upgrade google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8938e424-dcd3-4d95-b065-6f0a68b7550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "        summarize_s3_file_gem(\n",
    "            \"C3H1659897_Output.txt\",\n",
    "            template_key,\n",
    "            output_key\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167a7309-e05d-46ca-a58d-b77bdfabcf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sys\n",
    "s3 = boto3.client('s3')\n",
    "bucket = \"summery-prompts-data\"\n",
    "\n",
    "paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "for page in paginator.paginate(Bucket=bucket):\n",
    "    for obj in page.get(\"Contents\", []):\n",
    "        key = obj[\"Key\"]\n",
    "        if not key.lower().endswith((\".pdf\")):\n",
    "            continue\n",
    "        # extract the numeric ID (basename without extension)\n",
    "        file_id = key.rsplit(\"/\", 1)[-1].rsplit(\".\", 1)[0]\n",
    "\n",
    "        input_key  = key\n",
    "        output_key = f\"שינוי אחזקות בעלי עניין/Outputs/Z{file_id}text.txt\"\n",
    "\n",
    "        input_text = s3_read_text(input_key)\n",
    "        print(sys.getsizeof(input_text), \"bytes\")\n",
    "        #s3_write_text(output_key, input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2073e29e-d643-4944-840a-88442bcd9877",
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "try:\n",
    "    print(f\"genai module loaded from: {genai.__file__}\")\n",
    "    print(f\"genai version: {genai.__version__}\") # Check if version attribute exists and matches\n",
    "except AttributeError:\n",
    "    print(\"Could not determine genai module path or version easily.\")\n",
    "    print(f\"Type of genai: {type(genai)}\") # See what 'genai' actually is\n",
    "\n",
    "# Also check the path reported by the loader mechanism\n",
    "import importlib.util\n",
    "spec = importlib.util.find_spec(\"google.generativeai\")\n",
    "if spec:\n",
    "    print(f\"Spec location: {spec.origin}\")\n",
    "else:\n",
    "    print(\"Could not find spec for google.generativeai\")\n",
    "\n",
    "# And verify the Part attribute directly after import\n",
    "print(f\"Does genai have Part attribute? {'Part' in dir(genai)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e626e28-887f-4b83-801e-9cf12c796b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai.types as types\n",
    "\n",
    "# Check if the submodule itself exists and has attributes\n",
    "if hasattr(types, 'content_types'):\n",
    "    print(f\"types.content_types exists. Attributes: {dir(types.content_types)}\")\n",
    "    print(f\"Does types.content_types have Part attribute? {'Part' in dir(types.content_types)}\")\n",
    "else:\n",
    "    print(\"types.content_types submodule not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9c4cab7-772e-4b7d-a9ed-b4cc40ee940f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: /home/sagemaker-user/.conda/envs/gemini-env/bin/python\n",
      "Loaded genai version: 0.8.5\n",
      "After installing 0.7.1: Does genai have Part attribute? False\n",
      "Checking genai.types...\n",
      "Does genai.types have Part attribute? False\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import sys\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "try:\n",
    "    print(f\"Loaded genai version: {genai.__version__}\") # Should now hopefully report 0.7.1\n",
    "except AttributeError:\n",
    "    print(\"Could not read genai version.\")\n",
    "\n",
    "has_part = 'Part' in dir(genai)\n",
    "print(f\"After installing 0.7.1: Does genai have Part attribute? {has_part}\")\n",
    "\n",
    "if not has_part:\n",
    "    print(\"Checking genai.types...\")\n",
    "    import google.generativeai.types as types\n",
    "    print(f\"Does genai.types have Part attribute? {'Part' in dir(types)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffe6558-24ea-4ee0-8d0c-a63eec078510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import httpx\n",
    "client = genai.Client(api_key=\"AIzaSyA1NptQkNBxjgFG9Lop_NrfjVKq7-qg5HU\")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-1.5-flash-8b\", contents=\"Explain how AI works in a few words\"\n",
    ")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9f044a-bab4-4324-828a-4b3b91f42be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=\"YOUR_API_KEY\")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"Explain how AI works in a few words\",\n",
    ")\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "816304c0-2c91-4a6f-a6f6-760067b6b065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking availability of 'Part' using Python: /home/sagemaker-user/.conda/envs/gemini-env/bin/python\n",
      "Successfully imported 'google.generativeai'\n",
      "Detected google-generativeai version: 0.8.5\n",
      "\n",
      "Checking 'genai.Part'... Exists? False\n",
      "\n",
      "Checking 'genai.types.Part'... Exists? False\n",
      "\n",
      "--- Summary ---\n",
      "ERROR: 'Part' class was NOT found under 'genai' or 'genai.types'. Check library installation and version.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Checking availability of 'Part' using Python: {sys.executable}\")\n",
    "\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "    print(f\"Successfully imported 'google.generativeai'\")\n",
    "\n",
    "    # Check version for context\n",
    "    try:\n",
    "        print(f\"Detected google-generativeai version: {genai.__version__}\")\n",
    "    except AttributeError:\n",
    "        print(\"Could not determine google-generativeai version.\")\n",
    "\n",
    "    # --- Check 1: Directly under 'genai' (Expected for recent versions) ---\n",
    "    has_part_in_genai = hasattr(genai, 'Part')\n",
    "    print(f\"\\nChecking 'genai.Part'... Exists? {has_part_in_genai}\")\n",
    "    if has_part_in_genai:\n",
    "        print(\" -> You should be able to use 'genai.Part.from_uri(...)'\")\n",
    "\n",
    "    # --- Check 2: Under 'genai.types' (Less common/older versions) ---\n",
    "    try:\n",
    "        import google.generativeai.types as types\n",
    "        has_part_in_types = hasattr(types, 'Part')\n",
    "        print(f\"\\nChecking 'genai.types.Part'... Exists? {has_part_in_types}\")\n",
    "        if has_part_in_types:\n",
    "             print(\" -> You might need to use 'types.Part.from_uri(...)' (less common)\")\n",
    "    except ImportError:\n",
    "        print(\"\\nCould not import 'google.generativeai.types'.\")\n",
    "    except AttributeError:\n",
    "        # This might happen if 'types' exists but doesn't have 'Part'\n",
    "         print(f\"\\nChecking 'genai.types.Part'... Exists? False\")\n",
    "\n",
    "\n",
    "    # --- Summary ---\n",
    "    print(\"\\n--- Summary ---\")\n",
    "    if has_part_in_genai:\n",
    "        print(\"SUCCESS: 'Part' class found directly under 'genai'. Use 'genai.Part'.\")\n",
    "    elif 'has_part_in_types' in locals() and has_part_in_types:\n",
    "        print(\"INFO: 'Part' class found under 'genai.types'. Use 'types.Part'. This might indicate an older library version.\")\n",
    "    else:\n",
    "        print(\"ERROR: 'Part' class was NOT found under 'genai' or 'genai.types'. Check library installation and version.\")\n",
    "\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\nERROR: Failed to import the 'google.generativeai' library.\")\n",
    "    print(\"Please ensure it is installed correctly in this kernel's environment.\")\n",
    "    print(f\"Try running: !{sys.executable} -m pip install google-generativeai\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93114865-0ef7-400e-82e5-bbe5f1b9ff8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gemini-env)",
   "language": "python",
   "name": "gemini-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
